# File: build_vocab/build_vocab_vlsp.py

from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace
import os


def build_vlsp_vocab():
    # --- Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ---
    # 1. ÄÆ°á»ng dáº«n chá»©a file dá»¯ liá»‡u thÃ´ (Raw text)
    # DÃ¹ng os.path.join Ä‘á»ƒ trÃ¡nh lá»—i Window/Linux
    data_dir = os.path.join("..", "data", "vlsp_data")

    files = [
        os.path.join(data_dir, "train.vi.txt"),
        os.path.join(data_dir, "train.en.txt")
    ]

    # Kiá»ƒm tra xem file cÃ³ tá»“n táº¡i khÃ´ng trÆ°á»›c khi cháº¡y
    for f in files:
        if not os.path.exists(f):
            print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file táº¡i {f}")
            print(f"ğŸ‘‰ HÃ£y táº¡o thÆ° má»¥c 'data/vlsp_data' vÃ  copy file .txt vÃ o Ä‘Ã³!")
            return

    # 2. Cáº¥u hÃ¬nh Tokenizer
    tokenizer = Tokenizer(BPE(unk_token="<unk>"))
    tokenizer.pre_tokenizer = Whitespace()

    # vocab_size 10k lÃ  há»£p lÃ½ cho táº­p dá»¯ liá»‡u nhá»/chuyÃªn ngÃ nh
    trainer = BpeTrainer(
        vocab_size=10000,
        special_tokens=["<pad>", "<sos>", "<eos>", "<unk>"],
        # show_progress=True giÃºp báº¡n nhÃ¬n tháº¥y thanh tiáº¿n trÃ¬nh
        show_progress=True
    )

    # 3. Train
    print("ğŸš€ Äang train Tokenizer trÃªn dá»¯ liá»‡u Y táº¿ (VLSP)...")
    tokenizer.train(files, trainer)

    # 4. LÆ°u (LÆ°u vÃ o data/vlsp_vocab cho gá»n)
    save_path = os.path.join("..", "data", "vlsp_vocab", "tokenizer_shared.json")

    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    tokenizer.save(save_path)
    print(f"âœ… ÄÃ£ lÆ°u Tokenizer má»›i táº¡i: {save_path}")


if __name__ == "__main__":
    build_vlsp_vocab()