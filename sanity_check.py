import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from datasets import load_from_disk
import math
import time

# Import c√°c module t·ª´ project c·ªßa b·∫°n
# ƒê·∫£m b·∫£o b·∫°n ƒë·∫∑t file n√†y c√πng c·∫•p v·ªõi th∆∞ m·ª•c model/ v√† utils/
from model.transformer import Transformer
from utils.dataset import BilingualDataset, Collate
from utils.word_vocab import Vocabulary

def run_sanity_check():
    # 1. C·∫•u h√¨nh & Thi·∫øt b·ªã
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"--- SANITY CHECK STARTING ON {device} ---")

    # Hyperparameters (Gi·ªØ nguy√™n nh∆∞ train.py ƒë·ªÉ test ƒë√∫ng ki·∫øn tr√∫c)
    # D_MODEL = 512
    # N_LAYERS = 6
    # N_HEADS = 8
    # D_FF = 2048
    # DROPOUT = 0.0  # C√≥ th·ªÉ gi·∫£m v·ªÅ 0.0 ƒë·ªÉ test overfit nhanh h∆°n, nh∆∞ng ƒë·ªÉ 0.1 c≈©ng ƒë∆∞·ª£c
    # MAX_LEN = 100
    # BATCH_SIZE = 2  # Batch nh·ªè v√¨ ch·ªâ c√≥ 20 c√¢u
    # LEARNING_RATE = 0.005  # LR h∆°i cao m·ªôt ch√∫t ƒë·ªÉ h·ªôi t·ª• nhanh
    D_MODEL = 128
    N_LAYERS = 2
    N_HEADS = 4
    D_FF = 512
    DROPOUT = 0.0
    MAX_LEN = 100
    BATCH_SIZE = 20
    LEARNING_RATE = 1e-3

    N_EPOCHS = 300  # Ch·∫°y nhi·ªÅu epoch ƒë·ªÉ √©p loss v·ªÅ 0

    # 2. Load D·ªØ Li·ªáu & Vocab
    print("Loading Data & Vocab...")
    try:
        # ƒê∆∞·ªùng d·∫´n vocab (H√£y s·ª≠a l·∫°i n·∫øu kh√°c m√°y b·∫°n)
        vocab_src = Vocabulary.load_vocab("data/vocab0/vocab_src.json")
        vocab_tgt = Vocabulary.load_vocab("data/vocab0/vocab_tgt.json")

        # Load dataset
        dataset = load_from_disk("data/iwslt2015_data")

        # --- QUAN TR·ªåNG: CH·ªà L·∫§Y 20 C√ÇU ƒê·∫¶U TI√äN ---
        subset_size = 20
        raw_src = [item['vi'] for item in dataset['train']][:subset_size]
        raw_tgt = [item['en'] for item in dataset['train']][:subset_size]

        # --- SANITY CHECK C·ª®NG: l·∫∑p l·∫°i 1 c√¢u ---
        raw_src = [raw_src[1]] * 20
        raw_tgt = [raw_tgt[1]] * 20

        print(f"--> ƒê√£ l·∫•y {len(raw_src)} c·∫∑p c√¢u m·∫´u ƒë·ªÉ test.")
        print(f"--> V√≠ d·ª•: {raw_src[0]}  ==>  {raw_tgt[0]}")

    except Exception as e:
        print(f"L·ªói load data: {e}")
        return

    # T·∫°o Dataset & DataLoader
    sanity_dataset = BilingualDataset(raw_src, raw_tgt, vocab_src, vocab_tgt, max_len=MAX_LEN)
    collate = Collate(
        src_pad_idx=vocab_src.pad_idx,
        tgt_pad_idx=vocab_tgt.pad_idx
    )

    # Shuffle=True hay False kh√¥ng quan tr·ªçng v√¨ data qu√° √≠t, nh∆∞ng False d·ªÖ debug h∆°n
    sanity_iterator = DataLoader(sanity_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)

    # 3. Kh·ªüi t·∫°o Model
    model = Transformer(
        src_vocab_size=len(vocab_src),
        tgt_vocab_size=len(vocab_tgt),
        d_model=D_MODEL, n_layers=N_LAYERS, n_heads=N_HEADS, d_ff=D_FF,
        dropout=DROPOUT, max_len=5000,
        src_pad_idx=vocab_src.pad_idx, tgt_pad_idx=vocab_tgt.pad_idx
    ).to(device)

    # # Weight Tying (Gi·ªëng train.py)
    # model.src_embedding.emb.weight = model.tgt_embedding.emb.weight
    # model.fc_out.weight = model.src_embedding.emb.weight

    # 4. Optimizer & Loss
    # D√πng Adam th∆∞·ªùng thay v√¨ AdamW cho ƒë∆°n gi·∫£n, set LR c·ªë ƒë·ªãnh
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # T·∫Øt label smoothing ƒë·ªÉ √©p loss v·ªÅ 0 tuy·ªát ƒë·ªëi (Label smoothing s·∫Ω gi·ªØ loss quanh 1.0)
    criterion = nn.CrossEntropyLoss(ignore_index=vocab_tgt.pad_idx, label_smoothing=0.0)

    # 5. Training Loop si√™u ƒë∆°n gi·∫£n
    model.train()

    for epoch in range(N_EPOCHS):
        epoch_loss = 0

        for src, tgt in sanity_iterator:
            src, tgt = src.to(device), tgt.to(device)

            tgt_input = tgt[:, :-1]
            tgt_output = tgt[:, 1:]

            optimizer.zero_grad()

            output = model(src, tgt_input)

            # Reshape ƒë·ªÉ t√≠nh loss
            output_dim = output.shape[-1]
            output = output.contiguous().view(-1, output_dim)
            tgt_output = tgt_output.contiguous().view(-1)

            loss = criterion(output, tgt_output)
            loss.backward()

            # Clip grad ƒë·ªÉ an to√†n
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            optimizer.step()
            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(sanity_iterator)

        # In k·∫øt qu·∫£ m·ªói 10 epoch
        if (epoch + 1) % 10 == 0:
            ppl = math.exp(avg_loss) if avg_loss < 100 else float('inf')
            print(f"Epoch {epoch + 1:03d} | Loss: {avg_loss:.5f} | PPL: {ppl:.5f}")

            # N·∫øu Loss < 0.1 th√¨ th√†nh c√¥ng s·ªõm
            if avg_loss < 0.1:
                print(">>> TH√ÄNH C√îNG! Model ƒë√£ overfit ƒë∆∞·ª£c d·ªØ li·ªáu nh·ªè.")
                print(f"Testing th·ª≠ c√¢u ƒë·∫ßu ti√™n...")
                test_sanity_translation(model, raw_src[0], vocab_src, vocab_tgt, device)
                break

    print("--- HO√ÄN T·∫§T SANITY CHECK ---")


def test_sanity_translation(model, src_sentence, vocab_src, vocab_tgt, device, max_len=50):
    model.eval()

    # ===== Encode source =====
    src_ids = vocab_src.numericalize(src_sentence)
    src_ids = [vocab_src.sos_idx] + src_ids + [vocab_src.eos_idx]
    src_tensor = torch.tensor(src_ids).unsqueeze(0).to(device)

    # ===== Decode =====
    generated = [vocab_tgt.sos_idx]

    for _ in range(max_len):
        tgt_tensor = torch.tensor(generated).unsqueeze(0).to(device)

        with torch.no_grad():
            output = model(src_tensor, tgt_tensor)

        next_token = output[:, -1, :].argmax(dim=-1).item()

        # üîí B·∫£o v·ªá: token v∆∞·ª£t vocab
        if next_token >= len(vocab_tgt):
            print("‚ö†Ô∏è Token v∆∞·ª£t vocab, d·ª´ng decode.")
            break

        generated.append(next_token)

        if next_token == vocab_tgt.eos_idx:
            break

    # ===== Decode to text =====
    pred_sentence = vocab_tgt.decode(generated[1:])
    print("üîç Output:", pred_sentence)


if __name__ == "__main__":
    run_sanity_check()